{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "path = r'./UNSW-NB15/'\n",
    "\n",
    "features = pd.read_csv(path + \"NUSW-NB15_features.csv\", encoding='cp1252')\n",
    "\n",
    "data = pd.concat([\n",
    "    pd.read_csv(path + \"UNSW-NB15_1.csv\", low_memory=False, names=features.Name),\n",
    "    pd.read_csv(path + \"UNSW-NB15_2.csv\", low_memory=False, names=features.Name),\n",
    "    pd.read_csv(path + \"UNSW-NB15_3.csv\", low_memory=False, names=features.Name),\n",
    "    pd.read_csv(path + \"UNSW-NB15_4.csv\", low_memory=False, names=features.Name)\n",
    "])\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Conversion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features which are unusable in the real world are dropped\n",
    "\n",
    "the features are : `srcip`, `sport`, `dstip`\n",
    "\n",
    "ip and port can vary and they can be faked using vpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\n",
    "\tcolumns=[\"srcip\", \"sport\", \"dstip\"], \n",
    "\tinplace=True\n",
    ")\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing `ct_ftp_cmd` to int\n",
    "\n",
    "setting `-1` if the value was blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ct_ftp_cmd\"] = data[\"ct_ftp_cmd\"].apply(lambda x: int(x) if x != \" \" else -1)\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Hex to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"dsport\"] = data[\"dsport\"].apply(lambda x: int(x, 16) if x != \"-\" else -1)\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting `Nan` to `-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ct_flw_http_mthd\"].fillna(-1,  inplace=True)\n",
    "data[\"is_ftp_login\"].fillna(-1,  inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the nominal features\n",
    "\n",
    "todo: create the enum for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_features = [\"proto\", \"state\", \"service\"]\n",
    "\n",
    "for nominal_feature in nominal_features:\n",
    "\t\tnew_values, index = pd.factorize(data[nominal_feature])\n",
    "\t\tdata[nominal_feature] = new_values\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classication distributioon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_attack = pd.DataFrame(data.Label.value_counts())\n",
    "data_summary_attack.columns.values[0] = \"Occurrencies\"\n",
    "\n",
    "data_summary_attack[\"Percentage\"] = data.Label.value_counts() / len(data)\n",
    "\n",
    "data_summary_attack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very unbalanced dataset. Now let's look at the attack classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classificatin distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_summary_classes = pd.DataFrame(data.attack_cat.value_counts())\n",
    "data_summary_classes.columns.values[0] = \"Occurrencies\"\n",
    "\n",
    "attack_len = sum(data.Label)\n",
    "data_summary_classes[\"Percentage\"] = data.attack_cat.value_counts() / attack_len\n",
    "\n",
    "data_summary_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also the classes of attack are very unbalanced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the number of classes of attack\n",
    "\n",
    "Let's first see the clusterability of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclustertend as pyct\n",
    "\n",
    "#Calculating the clusterability of the data frame in multiple samples\n",
    "\n",
    "avg_list = []\n",
    "for sampling_size in range(10,51,10):\n",
    "    sample_list = []\n",
    "    for i in range(1,10,1):\n",
    "        data_blob = data.drop(columns=[\"Label\", \"attack_cat\"]).sample(20_000)\n",
    "        sample_list.append(pyct.hopkins(data_blob,sampling_size))\n",
    "    print(f'Sampled {sampling_size}: {sum(sample_list) / len(sample_list)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value is close to 0, so it'very clusterable\n",
    "\n",
    "Data is clustered using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "to_plot = data.dropna(subset=[\"attack_cat\"])\n",
    "\n",
    "to_plot = to_plot.drop(columns=[\"Label\", \"attack_cat\"]).sample(50_000)\n",
    "\n",
    "#to_plot = StandardScaler().fit_transform(to_plot)\n",
    "\n",
    "silhouette_list = []\n",
    "inertia_list=[]\n",
    "# f,axes = plt.subplots(2,4,figsize = (20,10))\n",
    "\n",
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=10, init=\"k-means++\")\n",
    "    y_pred = kmeans.fit_predict(to_plot)\n",
    "\n",
    "    # evaluate silhouette score\n",
    "    silhouetteavg = silhouette_score(to_plot,y_pred)\n",
    "    silhouette_list.append(silhouetteavg)\n",
    "\n",
    "    # evaluate inertia\n",
    "    inertia_list.append(kmeans.inertia_)\n",
    "\n",
    "    # display clustered samples\n",
    "    # axes[(n_clusters-2)//4][(n_clusters-2)%4].scatter(to_plot[:,0],to_plot[:,1], c = y_pred,alpha = 0.5)\n",
    "    # axes[(n_clusters-2)//4][(n_clusters-2)%4].axis('equal')\n",
    "    # axes[(n_clusters-2)//4][(n_clusters-2)%4].set_xlabel('Feature 1')\n",
    "    # axes[(n_clusters-2)//4][(n_clusters-2)%4].set_ylabel('Feature 2')\n",
    "    # axes[(n_clusters-2)//4][(n_clusters-2)%4].set_title(f'k={n_clusters} - Avg. Silhouette={silhouetteavg:.2} - n_iter = {kmeans.niter}' )\n",
    "\n",
    "    # display clusters centroids\n",
    "    centers = kmeans.cluster_centers_\n",
    "    # axes[(n_clusters-2)//4][(n_clusters-2)%4].scatter(centers[:,0],centers[:,1], marker = 'x',c = 'r')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plot silhouette and inertia trends w.r.t the number of clusters\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('avg-silhouette', color='black')\n",
    "ax1.plot(range(2,10),silhouette_list,'--ok')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "ax1.grid(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('loss', color='red')\n",
    "ax2.plot(range(2,10), inertia_list,'--or',alpha = 0.2)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.tight_layout()  # otherwise the right y-label is slightly clipped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the silhouette method we have that the optimal number of clusters is 7(?) (4-ish and 6-ish for only the attack categories), let's print the info of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster = data.drop(columns=\"Label\").sample(50_000)\n",
    "data_cluster = data_cluster.fillna({'attack_cat': \"NotAttack\"})\n",
    "data[\"attack_cat\"] = data[\"attack_cat\"].apply(lambda x: str(x))\n",
    "data_cluster.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster = data.drop(columns=\"Label\").sample(50_000)\n",
    "data_cluster = data_cluster.fillna({'attack_cat': \"NotAttack\"})\n",
    "data_cluster[\"attack_cat\"] = data_cluster[\"attack_cat\"].apply(lambda x: str(x))\n",
    "cluster = KMeans(n_clusters = 7, random_state = 10, init = \"k-means++\")\n",
    "data_cluster[\"cluster\"] = cluster.fit_predict(data_cluster.drop(columns=\"attack_cat\"))\n",
    "\n",
    "#data_cluster.cluster\n",
    "\n",
    "table = pd.pivot_table(data_cluster, index='attack_cat', columns='cluster', values='attack_cat', aggfunc=lambda x: 'count')\n",
    "print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then from each cluster we will select the predominat class of attack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the classes of attack are reduced to ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the features correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(40,32))\n",
    "\n",
    "data_without_label = data.drop(columns=['Label'])\n",
    "\n",
    "correlation_matrix = data_without_label.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, linewidths=0.5, fmt= '.2f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the over-correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "abs_correlation_matrix = correlation_matrix.abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = abs_correlation_matrix.where(np.triu(np.ones(abs_correlation_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "print(to_drop)\n",
    "\n",
    "# Drop features \n",
    "data.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain\n",
    "\n",
    "each features is analized to determine which features are not useful"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the predominant features distributions\n",
    "\n",
    "ex: if `unas` is used only to do attacks there is a real world problem, beacuse every time you use `unas` the classificator detects it as an attack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below we are cheking for the `proto` features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_protocol = pd.DataFrame(data.proto)\n",
    "data_summary_protocol.dropna(how=\"all\")\n",
    "data_summary_protocol = data_summary_protocol.reset_index().groupby(\"proto\").count()\n",
    "data_summary_protocol.columns.values[0] = \"Occurrencies\"\n",
    "data_summary_protocol[\"Percentage\"] = data_summary_protocol.Occurrencies/data_summary_protocol.sum()[0]\n",
    "data_summary_protocol.sort_values('Occurrencies', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the protocol occurrencies are mainly composed by tcp and udp, let's focus only on lines corresponding to the attack label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data[(data.Label == 1)]\n",
    "plt.figure()\n",
    "plt.hist(data_filtered.proto, bins = len(data_filtered.proto.value_counts()))\n",
    "plt.ylabel('Occurrences')\n",
    "plt.xlabel('Protocols')\n",
    "plt.xticks(rotation=45)\n",
    "data_summary_protocol_attack = pd.DataFrame(data_filtered.proto)\n",
    "data_summary_protocol_attack.dropna(how=\"all\")\n",
    "data_summary_protocol_attack = data_summary_protocol_attack.reset_index().groupby(\"proto\").count()\n",
    "data_summary_protocol_attack.columns.values[0] = \"Occurrencies\"\n",
    "data_summary_protocol_attack[\"Percentage\"] = data_summary_protocol_attack.Occurrencies/data_summary_protocol_attack.sum()[0]\n",
    "data_summary_protocol_attack.sort_values('Occurrencies', ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
