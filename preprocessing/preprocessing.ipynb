{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "path = r'./../data/UNSW-NB15/'\n",
    "\n",
    "features = pd.read_csv(path + \"NUSW-NB15_features.csv\", encoding='cp1252')\n",
    "\n",
    "data = pd.concat([\n",
    "    pd.read_csv(path + \"UNSW-NB15_1.csv\", low_memory=False, names=features.Name),\n",
    "    pd.read_csv(path + \"UNSW-NB15_2.csv\", low_memory=False, names=features.Name),\n",
    "    pd.read_csv(path + \"UNSW-NB15_3.csv\", low_memory=False, names=features.Name),\n",
    "    pd.read_csv(path + \"UNSW-NB15_4.csv\", low_memory=False, names=features.Name)\n",
    "])\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Conversion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features which are unusable in the real world are dropped\n",
    "\n",
    "the features are : `srcip`, `sport`, `dstip`\n",
    "\n",
    "ip and port can vary and they can be faked using vpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\n",
    "\tcolumns=[\"srcip\", \"sport\", \"dstip\"], \n",
    "\tinplace=True\n",
    ")\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing `ct_ftp_cmd` to int\n",
    "\n",
    "setting `-1` if the value was blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ct_ftp_cmd\"] = data[\"ct_ftp_cmd\"].apply(lambda x: int(x) if x != \" \" else -1)\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Hex to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"dsport\"] = data[\"dsport\"].apply(lambda x: int(x, 16) if x != \"-\" else -1)\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting `Nan` to `-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ct_flw_http_mthd\"].fillna(-1,  inplace=True)\n",
    "data[\"is_ftp_login\"].fillna(-1,  inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the nominal features\n",
    "\n",
    "todo: create the enum for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_features = [\"proto\", \"state\", \"service\"]\n",
    "\n",
    "for nominal_feature in nominal_features:\n",
    "\t\tnew_values, index = pd.factorize(data[nominal_feature])\n",
    "\t\tdata[nominal_feature] = new_values\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classication distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_attack = pd.DataFrame(data.Label.value_counts())\n",
    "data_summary_attack.columns.values[0] = \"Occurrencies\"\n",
    "\n",
    "data_summary_attack[\"Percentage\"] = data.Label.value_counts() / len(data)\n",
    "\n",
    "data_summary_attack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very unbalanced dataset. Now let's look at the attack classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classificatin distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_classes = pd.DataFrame(data.attack_cat.value_counts())\n",
    "data_summary_classes.columns.values[0] = \"Occurrencies\"\n",
    "\n",
    "attack_len = sum(data.Label)\n",
    "data_summary_classes[\"Percentage\"] = data.attack_cat.value_counts() / attack_len\n",
    "\n",
    "data_summary_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also the classes of attack are very unbalanced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the number of classes of attack\n",
    "\n",
    "Let's first see the clusterability of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclustertend as pyct\n",
    "\n",
    "#Calculating the clusterability of the data frame in multiple samples\n",
    "data2 = data[data[\"attack_cat\"] != 'nan'].dropna().drop_duplicates()\n",
    "avg_list = []\n",
    "for sampling_size in range(10,51,10):\n",
    "    sample_list = []\n",
    "    for i in range(1,10,1):\n",
    "        data_blob = data2.drop(columns=[\"Label\", \"attack_cat\"]).sample(20_000)\n",
    "        sample_list.append(pyct.hopkins(data_blob,sampling_size))\n",
    "    print(f'Sampled {sampling_size}: {sum(sample_list) / len(sample_list)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value is close to 0, so it'very clusterable\n",
    "\n",
    "Data is clustered using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "to_plot = data[data[\"attack_cat\"] != 'nan'].dropna().drop_duplicates()\n",
    "\n",
    "to_plot = to_plot.drop(columns=[\"Label\", \"attack_cat\"]).sample(50_000)\n",
    "\n",
    "#to_plot = StandardScaler().fit_transform(to_plot)\n",
    "\n",
    "silhouette_list = []\n",
    "inertia_list=[]\n",
    "\n",
    "for n_clusters in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    y_pred = kmeans.fit_predict(to_plot)\n",
    "\n",
    "    # evaluate silhouette score\n",
    "    silhouetteavg = silhouette_score(to_plot,y_pred)\n",
    "    silhouette_list.append(silhouetteavg)\n",
    "\n",
    "    # evaluate inertia\n",
    "    inertia_list.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plot silhouette and inertia trends w.r.t the number of clusters\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('avg-silhouette', color='black')\n",
    "ax1.plot(range(2,11),silhouette_list,'--ok')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "ax1.grid(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('loss', color='red')\n",
    "ax2.plot(range(2,11), inertia_list,'--or',alpha = 0.2)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.tight_layout()  # otherwise the right y-label is slightly clipped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the silhouette method we have that the optimal number of clusters is 4-ish and 6-ish for only the attack categories, let's print the info of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cluster = data[data[\"attack_cat\"] != 'nan'].drop(columns=\"Label\").dropna().sample(50_000)\n",
    "data_cluster = data[data[\"attack_cat\"] != 'nan'].drop(columns=\"Label\").dropna().drop_duplicates()\n",
    "data_cluster.attack_cat = data_cluster.attack_cat.apply(lambda x: x.strip())\n",
    "data_cluster.attack_cat = data_cluster.attack_cat.apply(lambda x: \"Backdoor\" if x == \"Backdoors\" else x)\n",
    "\n",
    "# Clustering with 4 clusters\n",
    "cluster1 = KMeans(n_clusters = 4)\n",
    "data_cluster[\"cluster\"] = cluster1.fit_predict(data_cluster.drop(columns=\"attack_cat\"))\n",
    "\n",
    "# conta il numero di elementi in ogni cluster\n",
    "counts = data_cluster.groupby('cluster')['attack_cat'].value_counts()\n",
    "\n",
    "#counts_df = pd.DataFrame(counts)\n",
    "table = pd.DataFrame()\n",
    "for i in range(0,4):\n",
    "    table[\"C \" + str(i)] = pd.DataFrame(counts[i])\n",
    "\n",
    "# print(table)\n",
    "# print(table.div(table.sum(axis=0), axis=1).round(4) * 100)\n",
    "print(table.div(table.sum(axis=1), axis=0).round(4) * 100)\n",
    "\n",
    "# Clustering with 7 clusters\n",
    "cluster2 = KMeans(n_clusters = 7)\n",
    "data_cluster[\"cluster\"] = cluster2.fit_predict(data_cluster.drop(columns=\"attack_cat\"))\n",
    "\n",
    "# conta il numero di elementi in ogni cluster\n",
    "counts = data_cluster.groupby('cluster')['attack_cat'].value_counts()\n",
    "\n",
    "#counts_df = pd.DataFrame(counts)\n",
    "table = pd.DataFrame()\n",
    "for i in range(0,7):\n",
    "    table[\"C \" + str(i)] = pd.DataFrame(counts[i])\n",
    "\n",
    "# print(table)\n",
    "# print(table.div(table.sum(axis=0), axis=1).round(4) * 100)\n",
    "print(table.div(table.sum(axis=1), axis=0).round(4) * 100)\n",
    "\n",
    "# Clustering with 9 clusters\n",
    "cluster3 = KMeans(n_clusters = 9)\n",
    "data_cluster[\"cluster\"] = cluster3.fit_predict(data_cluster.drop(columns=\"attack_cat\"))\n",
    "\n",
    "# conta il numero di elementi in ogni cluster\n",
    "counts = data_cluster.groupby('cluster')['attack_cat'].value_counts()\n",
    "\n",
    "#counts_df = pd.DataFrame(counts)\n",
    "table = pd.DataFrame()\n",
    "for i in range(0,9):\n",
    "    table[\"C \" + str(i)] = pd.DataFrame(counts[i])\n",
    "\n",
    "# print(table)\n",
    "# print(table.div(table.sum(axis=0), axis=1).round(4) * 100)\n",
    "print(table.div(table.sum(axis=1), axis=0).round(4) * 100)\n",
    "\n",
    "# Clustering with 15 clusters\n",
    "cluster4 = KMeans(n_clusters = 15)\n",
    "data_cluster[\"cluster\"] = cluster4.fit_predict(data_cluster.drop(columns=\"attack_cat\"))\n",
    "\n",
    "# conta il numero di elementi in ogni cluster\n",
    "counts = data_cluster.groupby('cluster')['attack_cat'].value_counts()\n",
    "\n",
    "#counts_df = pd.DataFrame(counts)\n",
    "table = pd.DataFrame()\n",
    "for i in range(0,15):\n",
    "    table[\"C \" + str(i)] = pd.DataFrame(counts[i])\n",
    "\n",
    "# print(table)\n",
    "# print(table.div(table.sum(axis=0), axis=1).round(4) * 100)\n",
    "print(table.div(table.sum(axis=1), axis=0).round(4) * 100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Shellcode] and [Reconnaissance] act similarly, the same goes for [Fuzzers] and [DoS], then [Analysis] and [Backdoor]. Then the [Worms] class can be eliminated because of the too few examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the classes of attack are reduced to [Shellcode/Reconnaissance], [Fuzzers/DoS], [Analysis/Backdoor], [Generic] and [Exploits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced = data[data['attack_cat'] != 'nan'].dropna().drop_duplicates()\n",
    "data_reduced = data_reduced[data_reduced['attack_cat'] != 'Worms']\n",
    "# print(data_reduced)\n",
    "\n",
    "data_reduced.attack_cat = data_reduced.attack_cat.apply(lambda x: str(x).strip())\n",
    "data_reduced.attack_cat = data_reduced.attack_cat.apply(lambda x: \"Shellcode/Reconnaissance\" if x == \"Shellcode\" or x == \"Reconnaissance\" else x)\n",
    "data_reduced.attack_cat = data_reduced.attack_cat.apply(lambda x: \"Fuzzers/DoS\" if x == \"Fuzzers\" or x == \"DoS\" else x)\n",
    "data_reduced.attack_cat = data_reduced.attack_cat.apply(lambda x: \"Analysis/Backdoor\" if x == \"Analysis\" or x == \"Backdoor\" or x == \"Backdoors\" else x)\n",
    "\n",
    "data_summary_classes = pd.DataFrame(data_reduced.attack_cat.value_counts())\n",
    "data_summary_classes.columns.values[0] = \"Occurrencies\"\n",
    "\n",
    "# attack_len = sum(data.Label)\n",
    "data_summary_classes[\"Percentage\"] = data_reduced.attack_cat.value_counts() / len(data_reduced)\n",
    "\n",
    "data_summary_classes\n",
    "# len(data_reduced)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding non attack rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_reduced,data[data.Label == 0].sample(100_000)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the features correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(40,32))\n",
    "\n",
    "data_without_label = data.drop(columns=['Label'])\n",
    "\n",
    "correlation_matrix = data_without_label.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, linewidths=0.5, fmt= '.2f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the over-correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "abs_correlation_matrix = correlation_matrix.abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = abs_correlation_matrix.where(np.triu(np.ones(abs_correlation_matrix.shape), k=1).astype(np.bool_))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "# print(to_drop)\n",
    "\n",
    "# Drop features \n",
    "data.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.info())\n",
    "f,ax = plt.subplots(figsize=(40,32))\n",
    "\n",
    "data_without_label = data.drop(columns=['Label'])\n",
    "\n",
    "correlation_matrix = data_without_label.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, linewidths=0.5, fmt= '.2f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export preprocessed data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./UNSW-NB15-preprocessed.csv\")\n",
    "\n",
    "data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "54d40a8d706b082090ba4f4c3f8e18179a1fa8c2ff26f9ae08651dbcb7b8ab81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
